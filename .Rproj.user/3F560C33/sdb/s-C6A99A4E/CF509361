{
    "contents" : "\n<!-- background: #b2cdb7 -->\n<!-- color: #fff -->\n<!-- font: din -->\n\nProject 1 - R Markdown\n========================================================\n\n\n### This is for the (Peer Reviewed) Project Assessment for the Practical Machine Learning Course from the John Hopkinson University\n\n#### **Author** - Mohit Sewak - *mohitsewak@gmail.com*\n\n##### Built on R version 2.15.2, using R Studio Version 0.98.507, Posted on GitHub Repository [mohitsewak/RMachineLearning] (http://mohitsewak.github.io/RMachineLearinng/) using gw-pages.\n\n\nLoading required libraries (Results Hidden, Warnings OFF, Messages OFF), and setting seed for reproducibility.\n\n```{r libraries, results='hide',warning=FALSE,message=FALSE}\nlibrary(caret)\nlibrary(randomForest)\nset.seed(1234)\n```\n\nClearing existing objects, Reading (& caching) Files, and making training and validation data sets.\n\n```{r reading_files, cache=TRUE}\nrm(list=ls())\ntrainingCSV = read.csv(\"pml-training.csv\")\ninTrain <- createDataPartition(trainingCSV$classe, p=0.60, list=FALSE)\ntraining <- trainingCSV[inTrain, ]\nvalidation <- trainingCSV[-inTrain, ]\n\n```\n\nCreating summary and viewing the top few records, and seeing paired scatter plots in the training file (Results Hidden, Output Cached).\n\n\n```{r data_summary, results=\"hide\", cache=TRUE}\nsummary(training)\nhead(training)\n```\n\nTrying to explore the data, esp the relations between variables. \n\n<p></p>\n\n<p> Since the number of features are huge, so most of the comprehensive plot based visualizations will break. Refer to the below error- Error: figure margins too large</p>\n\n<p></p>\n\n<p>There we may try to see multiple subsetted plot (as some examples given), or better still, see the tabular correlation matrix of the ones which are highly correlated.</p>\n\n```{r pair_plots, cache=TRUE,fig.width=7, fig.height=6, cache=TRUE}\nplot(training)\npairs(training[1:10000,1:10])\n```\n\nSince most of the columns have no data, or predictive power, it might not be conducive to use them as-is. Therefore filtering out fields with a lot of (more than 60%) null values.\n\n```{r preProcess, cache=TRUE}\ngoodVar<-c((colSums(is.na(training[,-160])) >= 0.4*nrow(training)),160)\ntraining<-training[,goodVar]\ndim(training)\nvalidation<-validation[,goodVar]\ndim(validation)\n\ntesting<-testing[,goodVar]\n\ntraining<-training[complete.cases(training),]\ndim(training)\n```\n\nTraining the model (RandomForest) on the training data set.\n\n```{r modeling, cache=TRUE}\nmodel <- randomForest(classe~.,data=training)\nprint(model)\nhead(importance(model))\n```\n\nEvaluating the model on the evaluation dataset.\n\n```{r validation, fig.width=7, fig.height=6, cache=TRUE}\nplot(predict(model,newdata=validation[,-ncol(validation)]),validation$classe)\nconfusionMatrix(predict(model,newdata=validation[,-ncol(validation)]),validation$classe)\n\naccurate<-c(as.numeric(predict(model,newdata=validation[,-ncol(validation)])==validation$classe))\naccuracy<-sum(accurate)*100/nrow(validation)\nmessage(\"Model Accuracy as tested over Validation set = \" , format(round(accuracy, 2), nsmall = 2), \"%\")\n```\n\nPredicting the new values in the testing csv provided.\n\n```{r testPrediction, cache=TRUE}\ntesting =  read.csv(\"pml-testing.csv\")\ndim(testing)\ntesting<-testing[,goodVar]\ndim(testing)\npredictions<-predict(model,newdata=testing)\npredictions\n```\n\n## Thank You for the Evaluation!!!\n### Best Regards,\n#### Mohit Sewak\n<p></p>\n",
    "created" : 1402679063868.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2651944277",
    "id" : "CF509361",
    "lastKnownWriteTime" : 1402739639,
    "path" : "~/Documents/GitHub/RMachineLearinng/PracticalMachineLearning_MohitSewak.Rmd",
    "project_path" : "PracticalMachineLearning_MohitSewak.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}